{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from decoder.transformer import TriplaneTransformer, BasicTransformerBlock\n",
    "from decoder.transformer_lora import TriplaneTransformer as TriplaneTransformer_lora\n",
    "model = TriplaneTransformer_lora(\n",
    "    inner_dim=1024, \n",
    "    num_layers=16, \n",
    "    num_heads=16,\n",
    "    image_feat_dim=768,\n",
    "    triplane_low_res=32, \n",
    "    triplane_high_res=64, \n",
    "    triplane_dim=80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load('transformer_ckpt.pth',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TriplaneTransformer(\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (cross_attn): CrossAttention(\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (k_proj): Linear(in_features=768, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=768, out_features=1024, bias=False)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attn): SelfAttention(\n",
       "        (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (deconv): ConvTranspose2d(1024, 80, kernel_size=(2, 2), stride=(2, 2))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import loralib as lora\n",
    "lora.mark_only_lora_as_trainable(model)\n",
    "# model.load_state_dict(ckpt)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lrm_generator.encoder.model.embeddings.cls_token',\n",
       " 'lrm_generator.encoder.model.embeddings.position_embeddings',\n",
       " 'lrm_generator.encoder.model.embeddings.patch_embeddings.projection.weight',\n",
       " 'lrm_generator.encoder.model.embeddings.patch_embeddings.projection.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.0.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.1.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.2.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.3.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.4.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.5.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.6.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.7.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.8.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.9.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.10.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.attention.attention.query.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.attention.attention.query.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.attention.attention.key.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.attention.attention.key.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.attention.attention.value.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.attention.attention.value.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.attention.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.attention.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.intermediate.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.intermediate.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.output.dense.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.output.dense.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.layernorm_before.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.layernorm_before.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.layernorm_after.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.layernorm_after.bias',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.adaLN_modulation.1.weight',\n",
       " 'lrm_generator.encoder.model.encoder.layer.11.adaLN_modulation.1.bias',\n",
       " 'lrm_generator.encoder.model.layernorm.weight',\n",
       " 'lrm_generator.encoder.model.layernorm.bias',\n",
       " 'lrm_generator.encoder.camera_embedder.0.weight',\n",
       " 'lrm_generator.encoder.camera_embedder.0.bias',\n",
       " 'lrm_generator.encoder.camera_embedder.2.weight',\n",
       " 'lrm_generator.encoder.camera_embedder.2.bias',\n",
       " 'lrm_generator.transformer.pos_embed',\n",
       " 'lrm_generator.transformer.layers.0.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.0.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.0.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.0.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.0.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.0.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.0.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.0.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.0.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.0.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.0.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.0.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.0.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.0.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.0.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.0.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.1.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.1.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.1.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.1.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.1.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.1.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.1.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.1.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.1.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.1.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.1.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.1.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.1.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.1.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.1.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.1.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.2.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.2.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.2.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.2.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.2.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.2.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.2.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.2.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.2.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.2.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.2.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.2.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.2.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.2.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.2.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.2.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.3.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.3.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.3.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.3.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.3.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.3.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.3.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.3.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.3.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.3.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.3.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.3.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.3.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.3.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.3.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.3.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.4.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.4.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.4.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.4.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.4.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.4.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.4.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.4.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.4.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.4.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.4.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.4.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.4.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.4.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.4.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.4.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.5.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.5.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.5.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.5.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.5.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.5.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.5.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.5.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.5.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.5.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.5.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.5.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.5.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.5.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.5.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.5.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.6.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.6.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.6.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.6.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.6.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.6.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.6.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.6.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.6.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.6.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.6.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.6.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.6.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.6.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.6.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.6.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.7.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.7.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.7.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.7.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.7.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.7.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.7.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.7.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.7.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.7.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.7.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.7.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.7.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.7.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.7.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.7.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.8.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.8.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.8.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.8.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.8.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.8.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.8.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.8.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.8.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.8.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.8.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.8.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.8.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.8.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.8.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.8.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.9.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.9.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.9.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.9.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.9.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.9.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.9.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.9.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.9.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.9.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.9.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.9.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.9.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.9.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.9.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.9.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.10.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.10.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.10.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.10.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.10.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.10.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.10.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.10.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.10.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.10.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.10.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.10.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.10.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.10.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.10.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.10.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.11.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.11.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.11.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.11.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.11.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.11.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.11.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.11.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.11.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.11.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.11.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.11.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.11.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.11.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.11.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.11.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.12.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.12.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.12.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.12.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.12.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.12.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.12.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.12.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.12.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.12.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.12.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.12.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.12.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.12.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.12.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.12.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.13.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.13.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.13.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.13.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.13.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.13.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.13.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.13.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.13.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.13.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.13.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.13.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.13.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.13.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.13.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.13.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.14.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.14.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.14.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.14.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.14.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.14.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.14.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.14.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.14.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.14.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.14.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.14.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.14.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.14.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.14.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.14.mlp.3.bias',\n",
       " 'lrm_generator.transformer.layers.15.norm1.weight',\n",
       " 'lrm_generator.transformer.layers.15.norm1.bias',\n",
       " 'lrm_generator.transformer.layers.15.cross_attn.q_proj_weight',\n",
       " 'lrm_generator.transformer.layers.15.cross_attn.k_proj_weight',\n",
       " 'lrm_generator.transformer.layers.15.cross_attn.v_proj_weight',\n",
       " 'lrm_generator.transformer.layers.15.cross_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.15.norm2.weight',\n",
       " 'lrm_generator.transformer.layers.15.norm2.bias',\n",
       " 'lrm_generator.transformer.layers.15.self_attn.in_proj_weight',\n",
       " 'lrm_generator.transformer.layers.15.self_attn.out_proj.weight',\n",
       " 'lrm_generator.transformer.layers.15.norm3.weight',\n",
       " 'lrm_generator.transformer.layers.15.norm3.bias',\n",
       " 'lrm_generator.transformer.layers.15.mlp.0.weight',\n",
       " 'lrm_generator.transformer.layers.15.mlp.0.bias',\n",
       " 'lrm_generator.transformer.layers.15.mlp.3.weight',\n",
       " 'lrm_generator.transformer.layers.15.mlp.3.bias',\n",
       " 'lrm_generator.transformer.norm.weight',\n",
       " 'lrm_generator.transformer.norm.bias',\n",
       " 'lrm_generator.transformer.deconv.weight',\n",
       " 'lrm_generator.transformer.deconv.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_sdf.0.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_sdf.0.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_sdf.2.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_sdf.2.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_sdf.4.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_sdf.4.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_sdf.6.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_sdf.6.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_rgb.0.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_rgb.0.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_rgb.2.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_rgb.2.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_rgb.4.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_rgb.4.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_rgb.6.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_rgb.6.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_deformation.0.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_deformation.0.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_deformation.2.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_deformation.2.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_deformation.4.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_deformation.4.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_deformation.6.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_deformation.6.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_weight.0.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_weight.0.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_weight.2.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_weight.2.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_weight.4.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_weight.4.bias',\n",
       " 'lrm_generator.synthesizer.decoder.net_weight.6.weight',\n",
       " 'lrm_generator.synthesizer.decoder.net_weight.6.bias']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\"/home/ubuntu/InstantMesh/ckpts/instant_mesh_large.ckpt\",map_location=\"cpu\")\n",
    "ckpt.keys()\n",
    "list(ckpt['state_dict'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\"/home/ubuntu/InstantMesh/ckpts/instant_mesh_large.ckpt\",map_location=\"cpu\")\n",
    "ckpt.keys()\n",
    "state_dict = ckpt['state_dict']\n",
    "\n",
    "# Filter and rename keys in the state_dict\n",
    "filtered_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if 'lrm_generator.transformer' in k:\n",
    "        new_key = k.replace(\"lrm_generator.transformer.\",\"\")\n",
    "        if k.endswith('in_proj_weight'):\n",
    "            new_key = new_key.replace('in_proj_weight', 'in_proj.weight')\n",
    "        if k.endswith('q_proj_weight'):\n",
    "            new_key = new_key.replace('q_proj_weight', 'q_proj.weight')\n",
    "        if k.endswith('k_proj_weight'):\n",
    "            new_key = new_key.replace('k_proj_weight', 'k_proj.weight')\n",
    "        if k.endswith('v_proj_weight'):\n",
    "            new_key = new_key.replace('v_proj_weight', 'v_proj.weight')\n",
    "        filtered_state_dict[new_key] = v\n",
    "\n",
    "torch.save(filtered_state_dict, 'transformer_ckpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos_embed',\n",
       " 'layers.0.norm1.weight',\n",
       " 'layers.0.norm1.bias',\n",
       " 'layers.0.cross_attn.q_proj.weight',\n",
       " 'layers.0.cross_attn.k_proj.weight',\n",
       " 'layers.0.cross_attn.v_proj.weight',\n",
       " 'layers.0.cross_attn.out_proj.weight',\n",
       " 'layers.0.norm2.weight',\n",
       " 'layers.0.norm2.bias',\n",
       " 'layers.0.self_attn.in_proj_weight',\n",
       " 'layers.0.self_attn.out_proj.weight',\n",
       " 'layers.0.norm3.weight',\n",
       " 'layers.0.norm3.bias',\n",
       " 'layers.0.mlp.0.weight',\n",
       " 'layers.0.mlp.0.bias',\n",
       " 'layers.0.mlp.3.weight',\n",
       " 'layers.0.mlp.3.bias',\n",
       " 'layers.1.norm1.weight',\n",
       " 'layers.1.norm1.bias',\n",
       " 'layers.1.cross_attn.q_proj.weight',\n",
       " 'layers.1.cross_attn.k_proj.weight',\n",
       " 'layers.1.cross_attn.v_proj.weight',\n",
       " 'layers.1.cross_attn.out_proj.weight',\n",
       " 'layers.1.norm2.weight',\n",
       " 'layers.1.norm2.bias',\n",
       " 'layers.1.self_attn.in_proj_weight',\n",
       " 'layers.1.self_attn.out_proj.weight',\n",
       " 'layers.1.norm3.weight',\n",
       " 'layers.1.norm3.bias',\n",
       " 'layers.1.mlp.0.weight',\n",
       " 'layers.1.mlp.0.bias',\n",
       " 'layers.1.mlp.3.weight',\n",
       " 'layers.1.mlp.3.bias',\n",
       " 'layers.2.norm1.weight',\n",
       " 'layers.2.norm1.bias',\n",
       " 'layers.2.cross_attn.q_proj.weight',\n",
       " 'layers.2.cross_attn.k_proj.weight',\n",
       " 'layers.2.cross_attn.v_proj.weight',\n",
       " 'layers.2.cross_attn.out_proj.weight',\n",
       " 'layers.2.norm2.weight',\n",
       " 'layers.2.norm2.bias',\n",
       " 'layers.2.self_attn.in_proj_weight',\n",
       " 'layers.2.self_attn.out_proj.weight',\n",
       " 'layers.2.norm3.weight',\n",
       " 'layers.2.norm3.bias',\n",
       " 'layers.2.mlp.0.weight',\n",
       " 'layers.2.mlp.0.bias',\n",
       " 'layers.2.mlp.3.weight',\n",
       " 'layers.2.mlp.3.bias',\n",
       " 'layers.3.norm1.weight',\n",
       " 'layers.3.norm1.bias',\n",
       " 'layers.3.cross_attn.q_proj.weight',\n",
       " 'layers.3.cross_attn.k_proj.weight',\n",
       " 'layers.3.cross_attn.v_proj.weight',\n",
       " 'layers.3.cross_attn.out_proj.weight',\n",
       " 'layers.3.norm2.weight',\n",
       " 'layers.3.norm2.bias',\n",
       " 'layers.3.self_attn.in_proj_weight',\n",
       " 'layers.3.self_attn.out_proj.weight',\n",
       " 'layers.3.norm3.weight',\n",
       " 'layers.3.norm3.bias',\n",
       " 'layers.3.mlp.0.weight',\n",
       " 'layers.3.mlp.0.bias',\n",
       " 'layers.3.mlp.3.weight',\n",
       " 'layers.3.mlp.3.bias',\n",
       " 'layers.4.norm1.weight',\n",
       " 'layers.4.norm1.bias',\n",
       " 'layers.4.cross_attn.q_proj.weight',\n",
       " 'layers.4.cross_attn.k_proj.weight',\n",
       " 'layers.4.cross_attn.v_proj.weight',\n",
       " 'layers.4.cross_attn.out_proj.weight',\n",
       " 'layers.4.norm2.weight',\n",
       " 'layers.4.norm2.bias',\n",
       " 'layers.4.self_attn.in_proj_weight',\n",
       " 'layers.4.self_attn.out_proj.weight',\n",
       " 'layers.4.norm3.weight',\n",
       " 'layers.4.norm3.bias',\n",
       " 'layers.4.mlp.0.weight',\n",
       " 'layers.4.mlp.0.bias',\n",
       " 'layers.4.mlp.3.weight',\n",
       " 'layers.4.mlp.3.bias',\n",
       " 'layers.5.norm1.weight',\n",
       " 'layers.5.norm1.bias',\n",
       " 'layers.5.cross_attn.q_proj.weight',\n",
       " 'layers.5.cross_attn.k_proj.weight',\n",
       " 'layers.5.cross_attn.v_proj.weight',\n",
       " 'layers.5.cross_attn.out_proj.weight',\n",
       " 'layers.5.norm2.weight',\n",
       " 'layers.5.norm2.bias',\n",
       " 'layers.5.self_attn.in_proj_weight',\n",
       " 'layers.5.self_attn.out_proj.weight',\n",
       " 'layers.5.norm3.weight',\n",
       " 'layers.5.norm3.bias',\n",
       " 'layers.5.mlp.0.weight',\n",
       " 'layers.5.mlp.0.bias',\n",
       " 'layers.5.mlp.3.weight',\n",
       " 'layers.5.mlp.3.bias',\n",
       " 'layers.6.norm1.weight',\n",
       " 'layers.6.norm1.bias',\n",
       " 'layers.6.cross_attn.q_proj.weight',\n",
       " 'layers.6.cross_attn.k_proj.weight',\n",
       " 'layers.6.cross_attn.v_proj.weight',\n",
       " 'layers.6.cross_attn.out_proj.weight',\n",
       " 'layers.6.norm2.weight',\n",
       " 'layers.6.norm2.bias',\n",
       " 'layers.6.self_attn.in_proj_weight',\n",
       " 'layers.6.self_attn.out_proj.weight',\n",
       " 'layers.6.norm3.weight',\n",
       " 'layers.6.norm3.bias',\n",
       " 'layers.6.mlp.0.weight',\n",
       " 'layers.6.mlp.0.bias',\n",
       " 'layers.6.mlp.3.weight',\n",
       " 'layers.6.mlp.3.bias',\n",
       " 'layers.7.norm1.weight',\n",
       " 'layers.7.norm1.bias',\n",
       " 'layers.7.cross_attn.q_proj.weight',\n",
       " 'layers.7.cross_attn.k_proj.weight',\n",
       " 'layers.7.cross_attn.v_proj.weight',\n",
       " 'layers.7.cross_attn.out_proj.weight',\n",
       " 'layers.7.norm2.weight',\n",
       " 'layers.7.norm2.bias',\n",
       " 'layers.7.self_attn.in_proj_weight',\n",
       " 'layers.7.self_attn.out_proj.weight',\n",
       " 'layers.7.norm3.weight',\n",
       " 'layers.7.norm3.bias',\n",
       " 'layers.7.mlp.0.weight',\n",
       " 'layers.7.mlp.0.bias',\n",
       " 'layers.7.mlp.3.weight',\n",
       " 'layers.7.mlp.3.bias',\n",
       " 'layers.8.norm1.weight',\n",
       " 'layers.8.norm1.bias',\n",
       " 'layers.8.cross_attn.q_proj.weight',\n",
       " 'layers.8.cross_attn.k_proj.weight',\n",
       " 'layers.8.cross_attn.v_proj.weight',\n",
       " 'layers.8.cross_attn.out_proj.weight',\n",
       " 'layers.8.norm2.weight',\n",
       " 'layers.8.norm2.bias',\n",
       " 'layers.8.self_attn.in_proj_weight',\n",
       " 'layers.8.self_attn.out_proj.weight',\n",
       " 'layers.8.norm3.weight',\n",
       " 'layers.8.norm3.bias',\n",
       " 'layers.8.mlp.0.weight',\n",
       " 'layers.8.mlp.0.bias',\n",
       " 'layers.8.mlp.3.weight',\n",
       " 'layers.8.mlp.3.bias',\n",
       " 'layers.9.norm1.weight',\n",
       " 'layers.9.norm1.bias',\n",
       " 'layers.9.cross_attn.q_proj.weight',\n",
       " 'layers.9.cross_attn.k_proj.weight',\n",
       " 'layers.9.cross_attn.v_proj.weight',\n",
       " 'layers.9.cross_attn.out_proj.weight',\n",
       " 'layers.9.norm2.weight',\n",
       " 'layers.9.norm2.bias',\n",
       " 'layers.9.self_attn.in_proj_weight',\n",
       " 'layers.9.self_attn.out_proj.weight',\n",
       " 'layers.9.norm3.weight',\n",
       " 'layers.9.norm3.bias',\n",
       " 'layers.9.mlp.0.weight',\n",
       " 'layers.9.mlp.0.bias',\n",
       " 'layers.9.mlp.3.weight',\n",
       " 'layers.9.mlp.3.bias',\n",
       " 'layers.10.norm1.weight',\n",
       " 'layers.10.norm1.bias',\n",
       " 'layers.10.cross_attn.q_proj.weight',\n",
       " 'layers.10.cross_attn.k_proj.weight',\n",
       " 'layers.10.cross_attn.v_proj.weight',\n",
       " 'layers.10.cross_attn.out_proj.weight',\n",
       " 'layers.10.norm2.weight',\n",
       " 'layers.10.norm2.bias',\n",
       " 'layers.10.self_attn.in_proj_weight',\n",
       " 'layers.10.self_attn.out_proj.weight',\n",
       " 'layers.10.norm3.weight',\n",
       " 'layers.10.norm3.bias',\n",
       " 'layers.10.mlp.0.weight',\n",
       " 'layers.10.mlp.0.bias',\n",
       " 'layers.10.mlp.3.weight',\n",
       " 'layers.10.mlp.3.bias',\n",
       " 'layers.11.norm1.weight',\n",
       " 'layers.11.norm1.bias',\n",
       " 'layers.11.cross_attn.q_proj.weight',\n",
       " 'layers.11.cross_attn.k_proj.weight',\n",
       " 'layers.11.cross_attn.v_proj.weight',\n",
       " 'layers.11.cross_attn.out_proj.weight',\n",
       " 'layers.11.norm2.weight',\n",
       " 'layers.11.norm2.bias',\n",
       " 'layers.11.self_attn.in_proj_weight',\n",
       " 'layers.11.self_attn.out_proj.weight',\n",
       " 'layers.11.norm3.weight',\n",
       " 'layers.11.norm3.bias',\n",
       " 'layers.11.mlp.0.weight',\n",
       " 'layers.11.mlp.0.bias',\n",
       " 'layers.11.mlp.3.weight',\n",
       " 'layers.11.mlp.3.bias',\n",
       " 'layers.12.norm1.weight',\n",
       " 'layers.12.norm1.bias',\n",
       " 'layers.12.cross_attn.q_proj.weight',\n",
       " 'layers.12.cross_attn.k_proj.weight',\n",
       " 'layers.12.cross_attn.v_proj.weight',\n",
       " 'layers.12.cross_attn.out_proj.weight',\n",
       " 'layers.12.norm2.weight',\n",
       " 'layers.12.norm2.bias',\n",
       " 'layers.12.self_attn.in_proj_weight',\n",
       " 'layers.12.self_attn.out_proj.weight',\n",
       " 'layers.12.norm3.weight',\n",
       " 'layers.12.norm3.bias',\n",
       " 'layers.12.mlp.0.weight',\n",
       " 'layers.12.mlp.0.bias',\n",
       " 'layers.12.mlp.3.weight',\n",
       " 'layers.12.mlp.3.bias',\n",
       " 'layers.13.norm1.weight',\n",
       " 'layers.13.norm1.bias',\n",
       " 'layers.13.cross_attn.q_proj.weight',\n",
       " 'layers.13.cross_attn.k_proj.weight',\n",
       " 'layers.13.cross_attn.v_proj.weight',\n",
       " 'layers.13.cross_attn.out_proj.weight',\n",
       " 'layers.13.norm2.weight',\n",
       " 'layers.13.norm2.bias',\n",
       " 'layers.13.self_attn.in_proj_weight',\n",
       " 'layers.13.self_attn.out_proj.weight',\n",
       " 'layers.13.norm3.weight',\n",
       " 'layers.13.norm3.bias',\n",
       " 'layers.13.mlp.0.weight',\n",
       " 'layers.13.mlp.0.bias',\n",
       " 'layers.13.mlp.3.weight',\n",
       " 'layers.13.mlp.3.bias',\n",
       " 'layers.14.norm1.weight',\n",
       " 'layers.14.norm1.bias',\n",
       " 'layers.14.cross_attn.q_proj.weight',\n",
       " 'layers.14.cross_attn.k_proj.weight',\n",
       " 'layers.14.cross_attn.v_proj.weight',\n",
       " 'layers.14.cross_attn.out_proj.weight',\n",
       " 'layers.14.norm2.weight',\n",
       " 'layers.14.norm2.bias',\n",
       " 'layers.14.self_attn.in_proj_weight',\n",
       " 'layers.14.self_attn.out_proj.weight',\n",
       " 'layers.14.norm3.weight',\n",
       " 'layers.14.norm3.bias',\n",
       " 'layers.14.mlp.0.weight',\n",
       " 'layers.14.mlp.0.bias',\n",
       " 'layers.14.mlp.3.weight',\n",
       " 'layers.14.mlp.3.bias',\n",
       " 'layers.15.norm1.weight',\n",
       " 'layers.15.norm1.bias',\n",
       " 'layers.15.cross_attn.q_proj.weight',\n",
       " 'layers.15.cross_attn.k_proj.weight',\n",
       " 'layers.15.cross_attn.v_proj.weight',\n",
       " 'layers.15.cross_attn.out_proj.weight',\n",
       " 'layers.15.norm2.weight',\n",
       " 'layers.15.norm2.bias',\n",
       " 'layers.15.self_attn.in_proj_weight',\n",
       " 'layers.15.self_attn.out_proj.weight',\n",
       " 'layers.15.norm3.weight',\n",
       " 'layers.15.norm3.bias',\n",
       " 'layers.15.mlp.0.weight',\n",
       " 'layers.15.mlp.0.bias',\n",
       " 'layers.15.mlp.3.weight',\n",
       " 'layers.15.mlp.3.bias',\n",
       " 'norm.weight',\n",
       " 'norm.bias',\n",
       " 'deconv.weight',\n",
       " 'deconv.bias']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filtered_state_dict.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instantmesh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
